{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB1Fm4kJ31qct0OCpSTDax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aravind8281/Natural_language_Processing/blob/main/Tensorflow_NLP_java.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4HhDdCHpkKPG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding using tensorflow"
      ],
      "metadata": {
        "id": "mhOJFp5OuHKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "corpus = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "input_sequences = []\n",
        "for sequence in sequences:\n",
        "    for i in range(1, len(sequence)):\n",
        "        n_gram_sequence = sequence[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "X = padded_sequences[:, :-1]\n",
        "y = tf.keras.utils.to_categorical(padded_sequences[:, -1], num_classes=total_words)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_words, output_dim=50, input_length=max_sequence_length-1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(total_words, activation=\"softmax\"))\n",
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=50, verbose=2)\n",
        "\n",
        "word_embeddings = model.layers[0].get_weights()[0]\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "for word, index in word_index.items():\n",
        "    if index < 10:\n",
        "        print(f\"{word}: {word_embeddings[index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMs88ymnzVg",
        "outputId": "91534270-3f87-42f1-da80-f249c5ab8fc5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 - 0s - loss: 2.3080 - accuracy: 0.0000e+00 - 396ms/epoch - 396ms/step\n",
            "Epoch 2/50\n",
            "1/1 - 0s - loss: 2.2977 - accuracy: 0.0000e+00 - 5ms/epoch - 5ms/step\n",
            "Epoch 3/50\n",
            "1/1 - 0s - loss: 2.2875 - accuracy: 0.1667 - 5ms/epoch - 5ms/step\n",
            "Epoch 4/50\n",
            "1/1 - 0s - loss: 2.2773 - accuracy: 0.1667 - 6ms/epoch - 6ms/step\n",
            "Epoch 5/50\n",
            "1/1 - 0s - loss: 2.2672 - accuracy: 0.2778 - 6ms/epoch - 6ms/step\n",
            "Epoch 6/50\n",
            "1/1 - 0s - loss: 2.2570 - accuracy: 0.4444 - 6ms/epoch - 6ms/step\n",
            "Epoch 7/50\n",
            "1/1 - 0s - loss: 2.2469 - accuracy: 0.4444 - 6ms/epoch - 6ms/step\n",
            "Epoch 8/50\n",
            "1/1 - 0s - loss: 2.2368 - accuracy: 0.4444 - 5ms/epoch - 5ms/step\n",
            "Epoch 9/50\n",
            "1/1 - 0s - loss: 2.2266 - accuracy: 0.5556 - 5ms/epoch - 5ms/step\n",
            "Epoch 10/50\n",
            "1/1 - 0s - loss: 2.2163 - accuracy: 0.6667 - 6ms/epoch - 6ms/step\n",
            "Epoch 11/50\n",
            "1/1 - 0s - loss: 2.2060 - accuracy: 0.7222 - 5ms/epoch - 5ms/step\n",
            "Epoch 12/50\n",
            "1/1 - 0s - loss: 2.1957 - accuracy: 0.6667 - 7ms/epoch - 7ms/step\n",
            "Epoch 13/50\n",
            "1/1 - 0s - loss: 2.1852 - accuracy: 0.6667 - 6ms/epoch - 6ms/step\n",
            "Epoch 14/50\n",
            "1/1 - 0s - loss: 2.1745 - accuracy: 0.6667 - 5ms/epoch - 5ms/step\n",
            "Epoch 15/50\n",
            "1/1 - 0s - loss: 2.1638 - accuracy: 0.6111 - 6ms/epoch - 6ms/step\n",
            "Epoch 16/50\n",
            "1/1 - 0s - loss: 2.1529 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 17/50\n",
            "1/1 - 0s - loss: 2.1418 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 18/50\n",
            "1/1 - 0s - loss: 2.1305 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 19/50\n",
            "1/1 - 0s - loss: 2.1191 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 20/50\n",
            "1/1 - 0s - loss: 2.1074 - accuracy: 0.6111 - 4ms/epoch - 4ms/step\n",
            "Epoch 21/50\n",
            "1/1 - 0s - loss: 2.0955 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 22/50\n",
            "1/1 - 0s - loss: 2.0833 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 23/50\n",
            "1/1 - 0s - loss: 2.0710 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 24/50\n",
            "1/1 - 0s - loss: 2.0583 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 25/50\n",
            "1/1 - 0s - loss: 2.0454 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 26/50\n",
            "1/1 - 0s - loss: 2.0323 - accuracy: 0.6111 - 7ms/epoch - 7ms/step\n",
            "Epoch 27/50\n",
            "1/1 - 0s - loss: 2.0189 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 28/50\n",
            "1/1 - 0s - loss: 2.0051 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 29/50\n",
            "1/1 - 0s - loss: 1.9912 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 30/50\n",
            "1/1 - 0s - loss: 1.9769 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 31/50\n",
            "1/1 - 0s - loss: 1.9623 - accuracy: 0.6111 - 6ms/epoch - 6ms/step\n",
            "Epoch 32/50\n",
            "1/1 - 0s - loss: 1.9475 - accuracy: 0.6111 - 6ms/epoch - 6ms/step\n",
            "Epoch 33/50\n",
            "1/1 - 0s - loss: 1.9323 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 34/50\n",
            "1/1 - 0s - loss: 1.9169 - accuracy: 0.6111 - 6ms/epoch - 6ms/step\n",
            "Epoch 35/50\n",
            "1/1 - 0s - loss: 1.9012 - accuracy: 0.6111 - 6ms/epoch - 6ms/step\n",
            "Epoch 36/50\n",
            "1/1 - 0s - loss: 1.8852 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 37/50\n",
            "1/1 - 0s - loss: 1.8689 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 38/50\n",
            "1/1 - 0s - loss: 1.8523 - accuracy: 0.6111 - 5ms/epoch - 5ms/step\n",
            "Epoch 39/50\n",
            "1/1 - 0s - loss: 1.8355 - accuracy: 0.6111 - 6ms/epoch - 6ms/step\n",
            "Epoch 40/50\n",
            "1/1 - 0s - loss: 1.8184 - accuracy: 0.6111 - 6ms/epoch - 6ms/step\n",
            "Epoch 41/50\n",
            "1/1 - 0s - loss: 1.8010 - accuracy: 0.6111 - 7ms/epoch - 7ms/step\n",
            "Epoch 42/50\n",
            "1/1 - 0s - loss: 1.7833 - accuracy: 0.6667 - 6ms/epoch - 6ms/step\n",
            "Epoch 43/50\n",
            "1/1 - 0s - loss: 1.7655 - accuracy: 0.6667 - 7ms/epoch - 7ms/step\n",
            "Epoch 44/50\n",
            "1/1 - 0s - loss: 1.7473 - accuracy: 0.6667 - 6ms/epoch - 6ms/step\n",
            "Epoch 45/50\n",
            "1/1 - 0s - loss: 1.7290 - accuracy: 0.6667 - 6ms/epoch - 6ms/step\n",
            "Epoch 46/50\n",
            "1/1 - 0s - loss: 1.7104 - accuracy: 0.6667 - 6ms/epoch - 6ms/step\n",
            "Epoch 47/50\n",
            "1/1 - 0s - loss: 1.6916 - accuracy: 0.6667 - 6ms/epoch - 6ms/step\n",
            "Epoch 48/50\n",
            "1/1 - 0s - loss: 1.6726 - accuracy: 0.7222 - 6ms/epoch - 6ms/step\n",
            "Epoch 49/50\n",
            "1/1 - 0s - loss: 1.6534 - accuracy: 0.7222 - 5ms/epoch - 5ms/step\n",
            "Epoch 50/50\n",
            "1/1 - 0s - loss: 1.6340 - accuracy: 0.7778 - 6ms/epoch - 6ms/step\n",
            "this: [-0.07787967  0.0596588   0.01549317  0.0574049   0.08759934  0.02408496\n",
            "  0.03397662 -0.0921096  -0.06573903 -0.07816977  0.06867591 -0.01457627\n",
            " -0.0734228  -0.02900896 -0.00871347 -0.05988844 -0.10732692 -0.03625616\n",
            "  0.02658623 -0.01361218 -0.05728918 -0.09236187  0.08238609 -0.0806499\n",
            " -0.0374466  -0.0333406   0.01348062 -0.06540372  0.01275202 -0.03474409\n",
            " -0.10054886  0.09863104  0.08396365 -0.0760371  -0.08365209  0.08133738\n",
            " -0.02524705 -0.04942438  0.02482353 -0.10824267 -0.03411081  0.09592817\n",
            " -0.04309077  0.06737944  0.02319936  0.08422861 -0.05677891  0.09425604\n",
            " -0.09928505  0.07738131]\n",
            "is: [-0.01896397  0.06361375 -0.02356389  0.08048786 -0.08206192  0.05182971\n",
            " -0.04801747 -0.04177427 -0.10219441 -0.02718504  0.02268084 -0.02028058\n",
            " -0.08292411 -0.01910227 -0.09544956 -0.04336622  0.09493086 -0.07416104\n",
            "  0.06476593 -0.00739536 -0.06376892 -0.09677847 -0.06752414  0.07319164\n",
            " -0.03057341 -0.00205947 -0.10485723 -0.08828809 -0.10516952  0.01440238\n",
            "  0.07964353  0.04830901  0.09768297  0.0360625   0.08084624  0.0358732\n",
            " -0.04912366 -0.07161788  0.03154925 -0.03546542 -0.10676328  0.0794798\n",
            " -0.01706805  0.05585868  0.06379218  0.0995832   0.04170856 -0.03734975\n",
            " -0.08383682  0.0481925 ]\n",
            "the: [ 0.03557943 -0.06376796  0.07356845 -0.06810861 -0.05660609 -0.08428445\n",
            " -0.08670472  0.04274668  0.02131241 -0.07919184  0.1061274   0.06634754\n",
            " -0.09580587  0.08964782  0.09551895 -0.04562972  0.06555896 -0.06786086\n",
            " -0.01493838  0.01628118 -0.07779554  0.01845831 -0.1007973   0.03782498\n",
            "  0.07952113 -0.08942628 -0.07378424 -0.02174329 -0.08688495 -0.05768618\n",
            "  0.06183859 -0.01381719 -0.02566496 -0.00451571 -0.09808924 -0.07457544\n",
            "  0.04030433  0.09792244 -0.06459892 -0.03741109 -0.02950475 -0.00300752\n",
            "  0.10070107 -0.01000954 -0.0962372  -0.04175618  0.02541511  0.00592558\n",
            "  0.07004633 -0.08286832]\n",
            "document: [-0.00596736  0.06734372 -0.06973717  0.07709957  0.08068804 -0.07434959\n",
            "  0.04054739 -0.07254517  0.05418001 -0.01712666 -0.07691824 -0.08348106\n",
            " -0.05908414 -0.09111919  0.08784407  0.10170102 -0.09266076 -0.07270632\n",
            " -0.076881   -0.02062686 -0.08813182 -0.00571232 -0.03129948 -0.02614475\n",
            " -0.0356255  -0.01096585  0.03693074  0.09792212  0.03298619  0.0652526\n",
            " -0.08170807  0.00284169  0.05882543 -0.07414451 -0.01833882 -0.035693\n",
            "  0.02146208  0.00241179 -0.07108761 -0.09380466  0.04436266  0.0088654\n",
            "  0.05107226  0.04716619 -0.06625472  0.01927864  0.07535953 -0.0892642\n",
            "  0.01660619  0.05807273]\n",
            "first: [-0.06267621  0.03340903  0.06122669 -0.00016021 -0.00398852  0.00186446\n",
            "  0.06225736  0.08680059 -0.08164499 -0.05181582  0.01196946 -0.09565737\n",
            " -0.09380938 -0.03456498 -0.05109997  0.10898737 -0.0705678   0.04764501\n",
            "  0.08012655  0.04777432  0.04150212  0.09078252  0.02215928  0.04419612\n",
            " -0.08303707  0.03736984  0.00956735  0.01901974  0.06575735 -0.10055114\n",
            " -0.00985153  0.05242884  0.0197947  -0.07236715 -0.00339658  0.01944197\n",
            " -0.08704381 -0.070522    0.04995279 -0.04116898  0.06776553 -0.04724743\n",
            " -0.09400228 -0.01445792  0.0158617  -0.06948695 -0.05100217 -0.02888018\n",
            " -0.03430153 -0.02270063]\n",
            "second: [-0.0117709   0.07831633  0.08907486 -0.04670915  0.01688438 -0.0006513\n",
            "  0.04882469  0.04542986 -0.09652737 -0.07823572  0.08225216 -0.02976434\n",
            " -0.06711099 -0.0443443  -0.09796854  0.08497704 -0.06453129  0.03377778\n",
            "  0.06310549  0.07166221  0.07800554  0.0970873   0.02749127  0.03306789\n",
            " -0.0146197   0.07808007  0.09862687  0.0656015   0.08754769 -0.0856052\n",
            " -0.08433083  0.04361091  0.06213748 -0.01474737 -0.02094962  0.10033561\n",
            " -0.08986059 -0.02015386  0.08368634 -0.10238732  0.01284433 -0.06294659\n",
            " -0.03219201 -0.09002183  0.01157649 -0.04575621 -0.04147891 -0.00573439\n",
            " -0.06223715 -0.05538495]\n",
            "and: [-0.02718873 -0.09938159 -0.04274081  0.05264966 -0.05135392 -0.00506793\n",
            "  0.01773236 -0.01663669  0.04428897  0.09025359  0.04963925 -0.01410928\n",
            "  0.02196064 -0.06757528  0.05909396  0.0982565   0.03608228 -0.05558038\n",
            " -0.08426891  0.04943053  0.10243834 -0.0491619  -0.00334374 -0.06952208\n",
            " -0.02274484  0.038004   -0.04991264 -0.09152345 -0.02690848  0.00656951\n",
            " -0.00156159  0.06480739  0.06631606  0.09308583 -0.08634051 -0.05805998\n",
            "  0.02134676  0.04711477  0.01659631 -0.02889245 -0.01437107 -0.06190808\n",
            "  0.0514808   0.05903521 -0.04030549 -0.07904037 -0.06825984 -0.092508\n",
            "  0.05080528  0.10703563]\n",
            "third: [ 3.77886556e-02  7.31305331e-02 -7.98566863e-02  4.15294729e-02\n",
            "  3.55868675e-02 -9.19334441e-02 -2.47027166e-02 -8.56879801e-02\n",
            " -8.54313746e-03  3.84717137e-02 -1.75573304e-02 -3.59839499e-02\n",
            "  1.07056692e-01  2.54425630e-02 -1.00033723e-01  2.34996211e-02\n",
            " -3.05520687e-02  9.33861062e-02  1.74802113e-02 -8.46816599e-02\n",
            " -1.09595042e-02  6.89333305e-02 -3.20397876e-02 -2.94068586e-02\n",
            "  9.89870653e-02 -2.61613540e-02  6.22980017e-03  2.26927660e-02\n",
            "  1.35896925e-03  4.76897247e-02 -9.27103404e-03  8.49383883e-04\n",
            " -7.96473920e-02  2.05193367e-02  4.54874672e-02 -1.80781242e-02\n",
            " -4.11168896e-02  7.62487471e-05 -3.01032159e-02  3.58976275e-02\n",
            "  1.99663043e-02  9.65291187e-02  1.00345954e-01 -8.82422458e-03\n",
            "  6.49903566e-02 -8.66428986e-02 -1.49914678e-02  7.20918998e-02\n",
            "  7.48863965e-02 -5.06922267e-02]\n",
            "one: [ 0.01839462 -0.02727727  0.03768326  0.04477808 -0.00402289  0.02960931\n",
            " -0.012379    0.00296425  0.02309377 -0.02082596 -0.01312338 -0.00948586\n",
            "  0.00547742 -0.02012613  0.04407103 -0.03531051  0.03628765 -0.00650524\n",
            " -0.00309219 -0.04043873 -0.04532119 -0.00887308  0.04692062  0.01051642\n",
            "  0.04800898  0.0246779  -0.03070575 -0.01725845 -0.00940322  0.00651592\n",
            " -0.03574523  0.01944118 -0.02759721 -0.00626386  0.0067394  -0.01671536\n",
            " -0.03536607  0.03064251  0.04501064  0.04783653 -0.00783014  0.04158671\n",
            " -0.02646161  0.01872246 -0.03542138  0.02627664 -0.03851576  0.01415583\n",
            "  0.02826631 -0.00789601]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation"
      ],
      "metadata": {
        "id": "OR-o4k6ZuiPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Sample text data\n",
        "corpus = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I am doing well.\",\n",
        "    \"What about you?\",\n",
        "    \"I'm just a computer program.\",\n",
        "    \"Nice to meet you!\",\n",
        "]\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences and labels\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "# Create predictors and labels\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"I am\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(\"Generated Text:\", seed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he7myhLvp8HP",
        "outputId": "f189a710-e2e1-4eea-c168-ec1b5389e787"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.9434 - accuracy: 0.0667\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.9381 - accuracy: 0.2000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9328 - accuracy: 0.2667\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.9274 - accuracy: 0.2000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.9219 - accuracy: 0.2000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.9162 - accuracy: 0.2000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.9103 - accuracy: 0.2000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9042 - accuracy: 0.2000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8977 - accuracy: 0.2000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8909 - accuracy: 0.2000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.8835 - accuracy: 0.2000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8757 - accuracy: 0.2000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8673 - accuracy: 0.2000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8582 - accuracy: 0.2000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8484 - accuracy: 0.2000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.8378 - accuracy: 0.2000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8262 - accuracy: 0.2000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8136 - accuracy: 0.2000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.7999 - accuracy: 0.2000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7848 - accuracy: 0.2000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7684 - accuracy: 0.2000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.7504 - accuracy: 0.2000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7308 - accuracy: 0.2000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.7094 - accuracy: 0.2000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6860 - accuracy: 0.2000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6607 - accuracy: 0.2000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6336 - accuracy: 0.2000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.6047 - accuracy: 0.2000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.5744 - accuracy: 0.2000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.5433 - accuracy: 0.2000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5124 - accuracy: 0.2000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4831 - accuracy: 0.2000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4566 - accuracy: 0.2000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4337 - accuracy: 0.2000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4136 - accuracy: 0.2000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3937 - accuracy: 0.2000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3714 - accuracy: 0.2000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3457 - accuracy: 0.2000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3172 - accuracy: 0.2000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2876 - accuracy: 0.2000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2586 - accuracy: 0.2000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2307 - accuracy: 0.2000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2041 - accuracy: 0.2000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1778 - accuracy: 0.2000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1509 - accuracy: 0.2000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1225 - accuracy: 0.2000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0921 - accuracy: 0.2000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0594 - accuracy: 0.2667\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0244 - accuracy: 0.3333\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9877 - accuracy: 0.3333\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9498 - accuracy: 0.4000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9112 - accuracy: 0.4667\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8721 - accuracy: 0.4667\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8319 - accuracy: 0.4667\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7900 - accuracy: 0.4667\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.7458 - accuracy: 0.5333\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6994 - accuracy: 0.5333\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.6518 - accuracy: 0.6000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6040 - accuracy: 0.6667\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5566 - accuracy: 0.6667\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5095 - accuracy: 0.7333\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4621 - accuracy: 0.8000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4143 - accuracy: 0.8000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3664 - accuracy: 0.8000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3192 - accuracy: 0.8000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2733 - accuracy: 0.8000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2282 - accuracy: 0.8000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1833 - accuracy: 0.8000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1387 - accuracy: 0.8000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0952 - accuracy: 0.8667\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0531 - accuracy: 0.8667\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0118 - accuracy: 0.8667\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9708 - accuracy: 0.9333\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9309 - accuracy: 0.9333\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8926 - accuracy: 0.9333\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8553 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8189 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7842 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7511 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7188 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6880 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6587 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6300 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6026 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5764 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5510 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5272 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5043 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4826 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4620 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4422 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4234 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4052 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3879 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3710 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3550 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.3394 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3244 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3100 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2962 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 353ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "Generated Text: I am doing well well well you you you you you you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "igSyRTvcn3n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the Seq2Seq model\n",
        "latent_dim = 256\n",
        "\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=latent_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "_, state_h, state_c = encoder_lstm(embedding_layer)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=latent_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(embedding_layer, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "Ms00a2SJvA5W",
        "outputId": "64477ec2-f2b2-46ee-b9c3-9781e830e784"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vocab_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8e16de736afb>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mencoder_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, MultiHeadAttention, Embedding, Dense\n",
        "\n",
        "class SelfAttention(Layer):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size needs to be divisible by heads\"\n",
        "\n",
        "        self.query = Dense(embed_size)\n",
        "        self.key = Dense(embed_size)\n",
        "        self.value = Dense(embed_size)\n",
        "        self.combine_heads = Dense(embed_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        query = self.query(inputs)  # (batch_size, seq_len, embed_size)\n",
        "        key = self.key(inputs)\n",
        "        value = self.value(inputs)\n",
        "\n",
        "        query = tf.reshape(\n",
        "            query, (batch_size, -1, self.heads, self.head_dim)\n",
        "        )  # (batch_size, seq_len, heads, head_dim)\n",
        "        key = tf.reshape(key, (batch_size, -1, self.heads, self.head_dim))\n",
        "        value = tf.reshape(value, (batch_size, -1, self.heads, self.head_dim))\n",
        "\n",
        "        query = tf.transpose(query, perm=[0, 2, 1, 3])  # (batch_size, heads, seq_len, head_dim)\n",
        "        key = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "        value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "\n",
        "        scores = tf.matmul(query, key, transpose_b=True)  # (batch_size, heads, seq_len, seq_len)\n",
        "        scores = scores / tf.math.sqrt(tf.cast(self.head_dim, dtype=tf.float32))\n",
        "\n",
        "        attention = tf.nn.softmax(scores, axis=3)\n",
        "\n",
        "        out = tf.matmul(attention, value)  # (batch_size, heads, seq_len, head_dim)\n",
        "        out = tf.transpose(out, perm=[0, 2, 1, 3])\n",
        "        out = tf.reshape(out, (batch_size, -1, self.embed_size))\n",
        "\n",
        "        out = self.combine_heads(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_size, heads, forward_expansion, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = Self\n"
      ],
      "metadata": {
        "id": "5tMtiTStvBWN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_translation_model(vocab_size, embedding_dim, hidden_units):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "    encoder_outputs, state_h, state_c = LSTM(hidden_units, return_state=True)(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention_layer = Attention()([decoder_outputs, encoder_outputs])\n",
        "\n",
        "    # Concatenate attention output and decoder LSTM output\n",
        "    attended_decoder = tf.concat([decoder_outputs, attention_layer], axis=-1)\n",
        "\n",
        "    # Dense layer for output predictions\n",
        "    decoder_dense = Dense(vocab_size, activation=\"softmax\")\n",
        "    decoder_outputs = decoder_dense(attended_decoder)\n",
        "\n",
        "    # Model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "vocab_size = 10000  # Adjust based on your dataset\n",
        "embedding_dim = 256\n",
        "hidden_units = 512\n",
        "\n",
        "translation_model = create_translation_model(vocab_size, embedding_dim, hidden_units)\n"
      ],
      "metadata": {
        "id": "PzSPyZM3vr9Q"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w5lQsUJJwJfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}