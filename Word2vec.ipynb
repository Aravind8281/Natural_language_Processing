{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwHzNRLpUvInQeuLgElG9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aravind8281/Natural_language_Processing/blob/main/Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding\n",
        "\n",
        "Word Embedding refers to the representation of words in a continuous vector space where the semantic meaning of words is captured based on their context. Word2Vec is a popular algorithm for creating word embeddings, and it provides a way to represent words as vectors in a high-dimensional space"
      ],
      "metadata": {
        "id": "USQNxx84J5ev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO0JOWgjIdUM",
        "outputId": "c0ac4a30-62e5-4458-e120-8b8b1228e34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-8.6312117e-03  3.6726298e-03  5.1969956e-03  5.7580448e-03\n",
            "  7.4746129e-03 -6.1871242e-03  1.1071811e-03  6.0690669e-03\n",
            " -2.8463460e-03 -6.1893342e-03 -4.0650315e-04 -8.3839893e-03\n",
            " -5.6131086e-03  7.1156803e-03  3.3478425e-03  7.2476044e-03\n",
            "  6.8202848e-03  7.5378688e-03 -3.7954499e-03 -5.7778065e-04\n",
            "  2.3588755e-03 -4.5184004e-03  8.3983373e-03 -9.8824166e-03\n",
            "  6.7796544e-03  2.9165917e-03 -4.9493043e-03  4.4116327e-03\n",
            " -1.7421589e-03  6.7185964e-03  9.9779824e-03 -4.3671504e-03\n",
            " -5.9382798e-04 -5.7030660e-03  3.8471050e-03  2.7969566e-03\n",
            "  6.9126366e-03  6.1113168e-03  9.5536355e-03  9.2885876e-03\n",
            "  7.9138782e-03 -6.9957683e-03 -9.1701644e-03 -3.6092810e-04\n",
            " -3.1049980e-03  7.9076663e-03  5.9406627e-03 -1.5445747e-03\n",
            "  1.5183457e-03  1.8017795e-03  7.8277048e-03 -9.5158573e-03\n",
            " -2.1121567e-04  3.4792917e-03 -9.3503715e-04  8.3848005e-03\n",
            "  9.0257991e-03  6.5426594e-03 -7.1442639e-04  7.7268728e-03\n",
            " -8.5508060e-03  3.2148394e-03 -4.6400535e-03 -5.0977739e-03\n",
            "  3.5963100e-03  5.3813951e-03  7.7815903e-03 -5.7854494e-03\n",
            "  7.4415938e-03  6.6306610e-03 -3.7173762e-03 -8.7600471e-03\n",
            "  5.4418924e-03  6.5079615e-03 -7.8546070e-04 -6.7292210e-03\n",
            " -7.0897057e-03 -2.5024153e-03  5.1546730e-03 -3.6692659e-03\n",
            " -9.3815522e-03  3.8377964e-03  4.8931455e-03 -6.4307363e-03\n",
            "  1.2078658e-03 -2.0773192e-03  2.5316300e-05 -9.8944996e-03\n",
            "  2.6969432e-03 -4.7506634e-03  1.0848022e-03 -1.5732157e-03\n",
            "  2.2079472e-03 -7.8876493e-03 -2.7121822e-03  2.6587201e-03\n",
            "  5.3402134e-03 -2.3878375e-03 -9.5206480e-03  4.5251912e-03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sentence=[\n",
        "    \"Word embeddings are powerful tools in natural language processing.\",\n",
        "    \"Word2Vec is a popular algorithm for generating word embeddings.\",\n",
        "    \"It captures semantic relationships between words.\",\n",
        "    \"Sentiment analysis benefits from using word embeddings.\"\n",
        "]\n",
        "token=[word_tokenize(word.lower()) for word in sentence]\n",
        "model=Word2Vec(sentences=token,vector_size=100,window=5,min_count=1,workers=4)\n",
        "model.save(\"model\")\n",
        "loaded=Word2Vec.load(\"model\")\n",
        "vector=loaded.wv['word']\n",
        "print(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CBOW\n",
        "Continuous Bag of Words (CBOW) is a type of Word2Vec model used in natural language processing to learn distributed representations of words. CBOW focuses on predicting a target word based on its context, meaning the words surrounding it"
      ],
      "metadata": {
        "id": "6gnVJHp9Pi_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence = \"Continuous Bag of Words (CBOW) is a type of Word2Vec model used in natural language processing.\"\n",
        "tokenized_sentence = word_tokenize(sentence.lower())\n",
        "tokenized_sentences = [tokenized_sentence]\n",
        "\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, min_count=1, workers=4)\n",
        "model.save(\"CBOW\")\n",
        "\n",
        "cbow_model = Word2Vec.load(\"CBOW\")\n"
      ],
      "metadata": {
        "id": "3WyHSnMKKPhn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "loaded_cbow_model = Word2Vec.load(\"CBOW_model\")\n",
        "missing_word_sentence = f\"I love __ language processing with {chosen_word}.\"\n",
        "tokenized_missing_word_sentence = word_tokenize(missing_word_sentence.lower())\n",
        "missing_word_index = tokenized_missing_word_sentence.index(\"__\")\n",
        "context_words = tokenized_missing_word_sentence[\n",
        "    max(0, missing_word_index - 2) : missing_word_index\n",
        "] + tokenized_missing_word_sentence[missing_word_index + 1 :]\n",
        "predicted_word = loaded_cbow_model.predict_output_word(context_words)\n",
        "print(f\"Predicted sentence: I love {predicted_word[0]} language processing with {chosen_word}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMU8oV4BQql2",
        "outputId": "1a2ed730-4481-45d4-8d4e-52663f6b69d1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted sentence: I love ('processing', 0.04000341) language processing with natural.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip Grams\n",
        "Skip gram helps to find the surrounding words"
      ],
      "metadata": {
        "id": "0fydlogaWp_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "corpus = \"Skip-gram is a type of Word2Vec model used for natural language processing tasks.\"\n",
        "tokenized_corpus = word_tokenize(corpus.lower())\n",
        "model = Word2Vec(sentences=[tokenized_corpus], vector_size=100, window=5, sg=1, min_count=1)\n",
        "vector_representation = model.wv['skip-gram']\n",
        "similar_words = model.wv.most_similar('skip-gram', topn=5)\n",
        "print(\"Vector representation of 'skip-gram':\", vector_representation)\n",
        "print(\"Similar words to 'skip-gram':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRPsdAiOTq04",
        "outputId": "2ae733fc-3dbc-4db0-e1b1-3b17ba9eb280"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector representation of 'skip-gram': [-0.00950053  0.00956329 -0.00777098 -0.00264749 -0.004909   -0.00496774\n",
            " -0.00802175 -0.00778174 -0.00455603 -0.00127801 -0.00510316  0.00614038\n",
            " -0.00951903 -0.00530913  0.00943649  0.00698812  0.00767476  0.00423312\n",
            "  0.00050608 -0.00598371  0.00601578  0.00263353  0.00770308  0.00639519\n",
            "  0.00793947  0.00865565 -0.00989587 -0.00675828  0.00133848  0.006442\n",
            "  0.00737591  0.00551461  0.0076603  -0.00512752  0.00658372 -0.00410538\n",
            " -0.00905433  0.00914211  0.00133122 -0.00276238 -0.00247521 -0.00422346\n",
            "  0.00481142  0.00439959 -0.00265342 -0.00734195 -0.00356582 -0.00033561\n",
            "  0.00609489 -0.0028371  -0.00011981  0.00087562 -0.0070936   0.00206835\n",
            " -0.00143702  0.00280175  0.00484492 -0.00135458 -0.00278321  0.00773507\n",
            "  0.00504821  0.00671288  0.00451923  0.00866465  0.00747495 -0.00107953\n",
            "  0.00874805  0.00460283  0.00543659 -0.001389   -0.00204127 -0.00442064\n",
            " -0.00851286  0.00304053  0.00888407  0.00892206 -0.00194521  0.00608881\n",
            "  0.00377916 -0.0042946   0.00204555 -0.00543944  0.0082089   0.00543018\n",
            "  0.00318484  0.00409932  0.00865604  0.00727336 -0.00083372 -0.00707117\n",
            "  0.00838336  0.00723584  0.00172656 -0.00134797 -0.00588645 -0.00453218\n",
            "  0.00864754 -0.00313867 -0.00633874  0.00986852]\n",
            "Similar words to 'skip-gram': [('natural', 0.25295141339302063), ('tasks', 0.1372442990541458), ('for', 0.04443884640932083), ('used', 0.012919014319777489), ('language', 0.006794902496039867)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negative Sampling\n",
        "\n",
        "Negative sampling is a technique used in word embedding models, particularly in algorithms like Word2Vec, to efficiently train the model by sampling a small number of negative examples (words that do not appear in the context) for each training instance. The goal is to distinguish between true context words and randomly sampled negative words, making the training process computationally more efficient."
      ],
      "metadata": {
        "id": "C4V3-lJuXq6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.models\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"I love natural language processing.\",\n",
        "    \"Word embeddings capture semantic relationships.\",\n",
        "    \"CBOW is a technique in NLP.\",\n",
        "    \"It learns from contextual words.\",\n",
        "    \"Training a CBOW model is straightforward.\"\n",
        "]\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokenized_corpus = [simple_preprocess(sentence, deacc=True) for sentence in corpus]\n",
        "\n",
        "# Create the Word2Vec model with negative sampling\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=100,\n",
        "    window=2,\n",
        "    sg=0,\n",
        "    min_count=1,\n",
        "    negative=15\n",
        ")\n",
        "target_word = \"processing\"\n",
        "context_words = [\"language\", \"natural\"]\n",
        "model.train([[target_word], context_words], epochs=model.epochs, total_examples=len(tokenized_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPY1jsxcVHgD",
        "outputId": "6f19e5e7-3c4f-44c2-a27f-67250dbe5ccc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
            "WARNING:gensim.models.word2vec:EPOCH 0: supplied example count (2) did not equal expected count (5)\n",
            "WARNING:gensim.models.word2vec:EPOCH 1: supplied example count (2) did not equal expected count (5)\n",
            "WARNING:gensim.models.word2vec:EPOCH 2: supplied example count (2) did not equal expected count (5)\n",
            "WARNING:gensim.models.word2vec:EPOCH 3: supplied example count (2) did not equal expected count (5)\n",
            "WARNING:gensim.models.word2vec:EPOCH 4: supplied example count (2) did not equal expected count (5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity and Analogy\n",
        "    Similarity: In the context of word embeddings like Word2Vec, similarity refers to the measure of closeness or relatedness between two words in the vector space. Similar words should have similar vector representations, indicating a shared semantic meaning.\n",
        "\n",
        "    Analogy: Analogy tasks involve finding a word that completes a given analogy relationship. For example, if \"man\" is to \"woman\" as \"king\" is to... (the expected answer is \"queen\"). Analogies showcase the ability of word embeddings to capture relationships between words."
      ],
      "metadata": {
        "id": "jI3qwkTXazR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import gensim.downloader as api\n",
        "\n",
        "path = api.load('word2vec-google-news-300', return_path=True)\n",
        "model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "\n",
        "def similarity(word1, word2):\n",
        "    similarity = model.similarity(word1, word2)\n",
        "    print(\"Similarity:\", similarity)\n",
        "\n",
        "def analogy(positive, negative, topn=1):\n",
        "    analogy_result = model.most_similar(positive=positive, negative=negative, topn=topn)\n",
        "    print(\"Analogy:\", analogy_result[0][0])\n",
        "\n",
        "similarity(\"king\", \"queen\")\n",
        "analogy(['man', 'queen'], ['woman'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46VAAxGLYVIq",
        "outputId": "154641b7-a468-4e82-a31d-12c522d6725f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: 0.6510957\n",
            "Analogy: king\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4iinCcQRas_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}